{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Lab 5: HuggingFace and Visual and Large Multimodal Models\n", "\n", "This lab focuses on how to utilize Visual or Large Multimodal Models (LMM), as well as demonstrating the use of various HuggingFace models and pipelines. \n", "\n", "We divide the lab into five notebooks.  You should reset the kernel each time before running a new notebook, to free up the GPU memory.  Otherwise you might get out of memory errors.  \n", "\n", "Note: The weights of these several models in this notebook are sourced from HuggingFace and have already been downloaded to **`/share/lab5`**, you can use them directly. \n", "\n", "We demonstrate these three models: \n", "\n", "1. Segment anything model (SAM) - this notebook\n", "    The Segment Anything Model (SAM) produces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image.   \n", "    (https://github.com/facebookresearch/segment-anything)\n", "\n", "2. Stable-diffusion: see 2-sd.ipynb\n", "    A latent text-to-image diffusion model.   \n", "    (https://github.com/CompVis/stable-diffusion)  We are using version 1.5, but please feel free to try 2.1 version (also available on the servers.)\n", "\n", "3. Fuyu - see 3-fuyu.ipynb\n", "    A small version of the multimodal model.  \n", "    The model is available on HuggingFace: (https://huggingface.co/adept/fuyu-8b) \n", "\n", "4. \uff08optional\uff09a text to video model - see 4-t2v.ipynb\n", "\n", "5. [Your task] We have downloaed a set of models in **`/share/LLMs`**, your task is to create a notebook to run at least one of them.  (You can search for the documenation on Hugging Face Hub).  If you want to download your own model from HuggingFace, please do so on your laptop, and then upload onto the cluster (instead of downloading it directly on the cluster as we have very limited Internet bandwidth.)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5.1 Segment Anything"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["# We have already downloaded the model for you, and the file is at `/share/lab5/sam`\n", "\n", "#!export HF_ENDPOINT=https://hf-mirror.com\n", "#!huggingface-cli download --resume-download facebook/sam-vit-huge --local-dir your_path_of_sam"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["import torch\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "#import cv2\n", "import gc\n", "from PIL import Image\n", "from transformers import pipeline"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["# open and show raw images\n", "# you can examine the directory for the images\n", "\n", "img_path = '/share/lab5/data/test3.jpg'\n", "raw_image = Image.open(img_path)\n", "raw_image.show()"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["# Create a mask-generation pipeline from the model /share/lab5/sam/, note the ending \"/\" in the path name\n", "\n", "generator = pipeline(\"mask-generation\", model=\"/share/lab5/sam/\", device=0)\n"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["# Using the pipeline to generate the mask\n", "\n", "outputs = generator(raw_image, points_per_batch=64)\n", "masks = outputs[\"masks\"]"]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": ["# Examine the mask data structure\n", "print(type(masks))\n", "print(masks[0].shape)\n", "masks[0]"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["## some utility functions to handle the masks\n", "\n", "def show_mask(mask, ax, random_color=True):\n", "    if random_color:\n", "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n", "    else:\n", "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n", "    h, w = mask.shape[-2:]\n", "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n", "    ax.imshow(mask_image)\n", "    del mask\n", "    gc.collect()\n", "\n", "def show_masks_on_image(raw_image, masks):\n", "  plt.imshow(np.array(raw_image))\n", "  ax = plt.gca()\n", "  ax.set_autoscale_on(False)\n", "  for mask in masks:\n", "    show_mask(mask, ax=ax, random_color=True)\n", "  plt.axis(\"off\")\n", "  plt.show()\n", "  del mask\n", "  gc.collect()"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["show_masks_on_image(raw_image, masks)"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["# Another example \n", "\n", "img_path = '/share/lab5/data/test8.jpg'\n", "raw_image = Image.open(img_path)\n", "raw_image.show()\n", "\n", "outputs = generator(raw_image, points_per_batch=64)\n", "masks = outputs[\"masks\"]\n", "show_masks_on_image(raw_image, masks)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#### Your Task ####\n", "# upload another image of your choice and generate the mask for it\n", "# show the entire mask"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#### Your Task ####\n", "# show only the mask with the largest area\n", "# hint: you need to examine the masks data structure to see how to calcluate the area.\n", "# if there are tow masks with the same area size, show either one\n"]}], "metadata": {"kernelspec": {"display_name": "base", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.13"}}, "nbformat": 4, "nbformat_minor": 2}