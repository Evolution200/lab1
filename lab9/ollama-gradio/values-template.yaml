########### First thing first ###########
NameSpace: you-forget-to-set-namespace   # 自己的namespace （同用户名）

########### vLLM Configuration ###########
# vllmPort: 8000
ContainerImage: harbor.ai.iiis.co/xuw/vllm:latest 

# Using the llama-3-8B-instruct model
ModelName: /ssdshare/Meta-Llama-3-8B-Instruct/

# Or using the Phi-3 model - you will need to add two additional arguments
# ModelName: /ssdshare/Phi-3-mini-128k-instruct/
# AdditionalArgs: | 
#     "--trust-remote-code", "--max-model-len", "7260",

# Resource for the vLLM server
Limits:  
 CPU: 16
 memory: 64Gi
 GPU: 2

# for running ollama locally, disable GFS use and set a local path to store the model parameters
# NoGFS: true
# LocalPath: YOUR_LOCAL_PATH

# Optional: Replace the default Command and Args

# vLLMCommand: '["python3", "-m", "vllm.entrypoints.openai.api_server"]'
# vLLMArgs: | 
#       ["--model", "/ssdshare/Meta-Llama-3-8B-Instruct/", 
#        "--dtype", "auto",
#        "--api-key", "$VLLM_API_KEY",
#        ]

########### Gradio Configuration ###########

## Replace with your domain name and gradio app docker image
IngressHost: YOUR_DNS_NAME
GradioImage: harbor.ai.iiis.co:9443/YOUR_IMAGE_NAME:VERSION
OpenAIUrl: http://openai-api:8000/v1

## Optional: Replace the default Command and Args
# GradioCommand: ["bash", "-c", "--"]
# GradioArgs: ["while true; do sleep 30; done;"]