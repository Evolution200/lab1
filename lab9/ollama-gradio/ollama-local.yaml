########### First thing first ###########
NameSpace: weixu   # 自己的namespace （同用户名）

########### vLLM Configuration ###########
vllmPort: 8000
ContainerImage: ollama:arm

# Using ollama 4bit quantized llama3-8B model
ModelName: llama3

# for local, disable GFS use
NoGFS: true
LocalPath: /Users/xuw/k8sshare/

# Using ollama 4bit quantized llama3-70B model
# ModelName: llama3:70b

# Resource for the vLLM server
Limits:  
 CPU: 6
 memory: 8Gi
 GPU: 0

# Optional: Replace the default Command and Args

# vLLMCommand: '["python3", "-m", "vllm.entrypoints.openai.api_server"]'
# vLLMArgs: | 
#       ["--model", "/ssdshare/Meta-Llama-3-8B-Instruct/", 
#        "--dtype", "auto",
#        "--api-key", "$VLLM_API_KEY",
#        ]

########### Gradio Configuration ###########

## Replace with your domain name and gradio app docker image
IngressHost: mylab9.ddns.net
GradioImage: harbor.ai.iiis.co:9443/xuw/lab9:v11
OpenAIUrl: http://openai-api:8000/v1

## Optional: Replace the default Command and Args
# GradioCommand: ["bash", "-c", "--"]
# GradioArgs: ["while true; do sleep 30; done;"]