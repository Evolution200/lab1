{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10.2 Accelerating Supervised Fine Tuning with Deepspeed\n",
    "## (Part III Run (distributed) SFT with Deepspeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dependencies and compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting trl (from -r requirements.txt (line 1))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/97/7e/274ed94ab7da21db4b7cbccad2bf2ed0940082a929b1512e508351b289f5/trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m337.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fire (from -r requirements.txt (line 2))\n",
      "  Using cached fire-0.6.0-py2.py3-none-any.whl\n",
      "Collecting bitsandbytes (from -r requirements.txt (line 3))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/2f/a4/d8c8c1f69ceb3afdc285d62c65bec8d46900d70e81c9a8b24883001e23f8/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m331.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:10\u001b[0m\n",
      "\u001b[?25hCollecting deepspeed (from -r requirements.txt (line 4))\n",
      "  Using cached deepspeed-0.14.2-py3-none-any.whl\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.9.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl->-r requirements.txt (line 1)) (4.38.1)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl->-r requirements.txt (line 1)) (1.26.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl->-r requirements.txt (line 1)) (0.27.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl->-r requirements.txt (line 1)) (2.17.1)\n",
      "Collecting tyro>=0.5.11 (from trl->-r requirements.txt (line 1))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/11/9d/bad5061876ee331cd2fc23f7a1fdbdb13ea2d65738fbb4e354b3ba644865/tyro-0.8.4-py3-none-any.whl (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m326.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->-r requirements.txt (line 2)) (1.16.0)\n",
      "Collecting termcolor (from fire->-r requirements.txt (line 2))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/d9/5f/8c716e47b3a50cbd7c146f45881e11d9414def768b7cd9c5e6650ec2a80a/termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting hjson (from deepspeed->-r requirements.txt (line 4))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/1f/7f/13cd798d180af4bf4c0ceddeefba2b864a63c71645abc0308b768d67bb81/hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m307.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting ninja (from deepspeed->-r requirements.txt (line 4))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/6d/92/8d7aebd4430ab5ff65df2bfee6d5745f95c004284db2d8ca76dcbfd9de47/ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m334.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 4)) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 4)) (5.9.0)\n",
      "Collecting py-cpuinfo (from deepspeed->-r requirements.txt (line 4))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/e0/a9/023730ba63db1e494a271cb018dcd361bd2c917ba7004c3e49d5daf795a2/py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 4)) (2.6.3)\n",
      "Collecting pynvml (from deepspeed->-r requirements.txt (line 4))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/5b/9c/adb8070059caaa15d5a572b66bccd95900d8c1b9fa54d6ecea6ae97448d1/pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m327.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 4)) (4.65.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft->-r requirements.txt (line 5)) (6.0.1)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft->-r requirements.txt (line 5)) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft->-r requirements.txt (line 5)) (0.21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r requirements.txt (line 5)) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r requirements.txt (line 5)) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r requirements.txt (line 5)) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r requirements.txt (line 5)) (4.10.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r requirements.txt (line 1)) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r requirements.txt (line 1)) (0.15.2)\n",
      "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl->-r requirements.txt (line 1))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/d5/7c/e9fcff7623954d86bdc17782036cbf715ecab1bec4847c008557affe1ca8/docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl->-r requirements.txt (line 1))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/87/67/a37f6214d0e9fe57f6ae54b2956d550ca8365857f42a1ce0392bb21d9410/rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m341.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl->-r requirements.txt (line 1))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/e2/d1/a1d3189e7873408b9dc396aef0d7926c198b0df2aa3ddb5b539d3e89a70f/shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 1)) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 1)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 1)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 1)) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 1)) (3.9.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed->-r requirements.txt (line 4)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed->-r requirements.txt (line 4)) (2.16.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r requirements.txt (line 1)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r requirements.txt (line 1)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r requirements.txt (line 1)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r requirements.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r requirements.txt (line 5)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r requirements.txt (line 5)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r requirements.txt (line 5)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r requirements.txt (line 5)) (2023.7.22)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl->-r requirements.txt (line 1))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m303.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl->-r requirements.txt (line 1)) (2.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r requirements.txt (line 1)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl->-r requirements.txt (line 1)) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl->-r requirements.txt (line 1))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: py-cpuinfo, ninja, hjson, termcolor, shtab, pynvml, mdurl, docstring-parser, markdown-it-py, fire, rich, deepspeed, bitsandbytes, tyro, trl\n",
      "Successfully installed bitsandbytes-0.43.1 deepspeed-0.14.2 docstring-parser-0.16 fire-0.6.0 hjson-3.1.0 markdown-it-py-3.0.0 mdurl-0.1.2 ninja-1.11.1.1 py-cpuinfo-9.0.0 pynvml-11.5.0 rich-13.7.1 shtab-1.7.1 termcolor-2.4.0 trl-0.8.6 tyro-0.8.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create the training script\n",
    "In this section, we will finetune a chat model using parallel technologies. We choose Llama-chat-7B as our base model and our dataset is code instrcutions, which guides LLM to write programming code from given natural language. \n",
    "Recall what we have learned in the lab8, now we put all steps into the launch script `launch_sft_llama2.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/ds_zero2_no_offload.json\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/ds_zero2_no_offload.json\n",
    "\n",
    "{\n",
    "    \"bf16\": {\n",
    "        \"enabled\": \"auto\"\n",
    "      },\n",
    "\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"allgather_partitions\": true,\n",
    "        \"allgather_bucket_size\": 1e8,\n",
    "        \"overlap_comm\": true,\n",
    "        \"reduce_scatter\": true,\n",
    "        \"reduce_bucket_size\": 1e8,\n",
    "        \"contiguous_gradients\": true\n",
    "    },\n",
    "    \n",
    "    \"checkpoint\": {\n",
    "        \"use_node_local_storage\": true\n",
    "      },\n",
    "\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": \"auto\",\n",
    "    \"steps_per_print\": 2000,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"wall_clock_breakdown\": false\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/train.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/train.py\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "################################################################################\n",
    "# Model name and directories\n",
    "################################################################################\n",
    "\n",
    "# The base model\n",
    "model_name = \"/share/model/llama-2-7b-chat-hf\"\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"/root/tmp/py18k\"\n",
    "# Fine-tuned model name\n",
    "new_model = \"/scratch2/llama-2-7b-py18k\"\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"/scratch2/results\"\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,    # use 4-bit precision for base model loading\n",
    "    bnb_4bit_quant_type= \"nf4\",  # Quantization type (fp4 or nf4)\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,   # Compute dtype for 4-bit base models  \"float16\" or torch.bfloat16\n",
    "    bnb_4bit_use_double_quant= False,  # Activate nested quantization for 4-bit base models (double quantization)\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "\n",
    "device_map = {'':local_rank}\n",
    "\n",
    "print(\"device map: \", device_map)\n",
    "\n",
    "# Load base model with bnb config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "bias=\"none\"\n",
    "task_type=\"CAUSAL_LM\"\n",
    "\n",
    "################################################################################\n",
    "# Training parameters (passed to TrainingArguments)\n",
    "################################################################################\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "# max_steps = 100\n",
    "# Enable fp16/bf16 training (set bf16 to True if supported by your GPU)\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "################################################################################\n",
    "# Monitoring parameters\n",
    "################################################################################\n",
    "\n",
    "# Logging dir (for tensorboard)\n",
    "logging_dir = f\"{output_dir}/logs\"\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "# Monitoring and Visualizing tools\n",
    "report_to = \"tensorboard\"\n",
    "\n",
    "################################################################################\n",
    "# SFTTrainer parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = 512\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(dataset_name)\n",
    "print(dataset[0])\n",
    "\n",
    "\n",
    "from peft import PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    deepspeed=\"/tmp/ds_zero2_no_offload.json\",  ## point to the deepspeed file generated in the previous step\n",
    "    #max_steps=max_steps,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    logging_steps=logging_steps,\n",
    "    logging_dir=logging_dir,\n",
    "    report_to=report_to, \n",
    ")\n",
    "\n",
    "from peft import LoraConfig\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=bias,\n",
    "    task_type=task_type,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "# # Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "\n",
    "\n",
    "# release GPU memory here\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can start the tensorboard for monitoring before training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we use deepspeed to speed up training, you can find details of deepspeed configuration in the `ds_zero2_no_offload.json`. \n",
    "You can adjust the number of GPUs for training according to the number of available GPUs on your POD by changing `nproc_per_node` and adjust the total number of POD invovled in the training by changing `nnodes`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running the training on a single machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single pod multiple GPUs\n",
    "# run the following command in the terminal\n",
    "# NCCL_P2P_DISABLE=1 NCCL_IB_DISABLE=1 torchrun --nnodes 1 --nproc_per_node 2 /tmp/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the tensorboard to monitor the training process, as before\n",
    "\n",
    "# tensorboard --logdir /scratch2/results/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running the script in a distributed way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start training on multiple pods, select one of your pods as the master node and check its IP of the infiniband NIC using `ifconfig`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cali27c024edcba: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1450\n",
      "        inet6 fe80::ecee:eeff:feee:eeee  prefixlen 64  scopeid 0x20<link>\n",
      "        ether ee:ee:ee:ee:ee:ee  txqueuelen 1000  (Ethernet)\n",
      "        RX packets 57  bytes 4086 (4.0 KB)\n",
      "        RX errors 0  dropped 0  overruns 0  frame 0\n",
      "        TX packets 68  bytes 7749 (7.7 KB)\n",
      "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
      "\n",
      "cali783eeef06f8: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1450\n",
      "        inet6 fe80::ecee:eeff:feee:eeee  prefixlen 64  scopeid 0x20<link>\n",
      "        ether ee:ee:ee:ee:ee:ee  txqueuelen 1000  (Ethernet)\n",
      "        RX packets 10982943  bytes 814076038 (814.0 MB)\n",
      "        RX errors 0  dropped 0  overruns 0  frame 0\n",
      "        TX packets 16100644  bytes 35129496214 (35.1 GB)\n",
      "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
      "\n",
      "cali9645486a5d5: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1450\n",
      "        inet6 fe80::ecee:eeff:feee:eeee  prefixlen 64  scopeid 0x20<link>\n",
      "        ether ee:ee:ee:ee:ee:ee  txqueuelen 1000  (Ethernet)\n",
      "        RX packets 4061328  bytes 362269720 (362.2 MB)\n",
      "        RX errors 0  dropped 0  overruns 0  frame 0\n",
      "        TX packets 4449721  bytes 7913010248 (7.9 GB)\n",
      "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
      "\n",
      "calibbb05c6f446: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1450\n",
      "        inet6 fe80::ecee:eeff:feee:eeee  prefixlen 64  scopeid 0x20<link>\n",
      "        ether ee:ee:ee:ee:ee:ee  txqueuelen 1000  (Ethernet)\n",
      "        RX packets 1115  bytes 78146 (78.1 KB)\n",
      "        RX errors 0  dropped 0  overruns 0  frame 0\n",
      "        TX packets 1154  bytes 123899 (123.8 KB)\n",
      "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
      "\n",
      "docker0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500\n",
      "        inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255\n",
      "        ether 02:42:11:d6:90:cf  txqueuelen 0  (Ethernet)\n",
      "        RX packets 0  bytes 0 (0.0 B)\n",
      "        RX errors 0  dropped 0  overruns 0  frame 0\n",
      "        TX packets 0  bytes 0 (0.0 B)\n",
      "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
      "\n",
      "ens11: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\n",
      "        inet 10.5.0.29  netmask 255.255.0.0  broadcast 10.5.255.255\n",
      "        inet6 fe80::f652:14ff:fe11:7700  prefixlen 64  scopeid 0x20<link>\n",
      "        ether f4:52:14:11:77:00  txqueuelen 1000  (Ethernet)\n",
      "        RX packets 455223798  bytes 579161596085 (579.1 GB)\n",
      "        RX errors 0  dropped 0  overruns 0  frame 0\n",
      "        TX packets 356501830  bytes 436102096705 (436.1 GB)\n",
      "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
      "\n",
      "ens111f0np0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500\n",
      "        ether b8:3f:d2:87:68:5e  txqueuelen 1000  (Ethernet)\n",
      "        RX packets 0  bytes 0 (0.0 B)\n",
      "        RX errors 0  dropped 0  overruns 0  frame 0\n",
      "        TX packets 0  bytes 0 (0.0 B)\n",
      "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
      "\n",
      "ens111f1np1: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500\n",
      "        ether b8:3f:d2:87:68:5f  txqueuelen 1000  (Ethernet)\n",
      "        RX packets 0  bytes 0 (0.0 B)\n",
      "        RX errors 0  dropped 0  overruns 0  frame 0\n",
      "        TX packets 0  bytes 0 (0.0 B)\n",
      "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
      "\n",
      "ens11d1: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500\n",
      "        ether f4:52:14:11:77:01  txqueuelen 1000  (Ethernet)\n",
      "        RX packets 0  bytes 0 (0.0 B)\n",
      "        RX errors 0  dropped 0  overruns 0  frame 0\n",
      "        TX packets 0  bytes 0 (0.0 B)\n",
      "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
      "\n",
      "ens121f0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\n",
      "        inet 10.1.0.29  netmask 255.255.0.0  broadcast 10.1.255.255\n",
      "        inet6 fe80::8261:5fff:fe22:4a03  prefixlen 64  scopeid 0x20<link>\n",
      "        ether 80:61:5f:22:4a:03  txqueuelen 1000  (Ethernet)\n",
      "        RX packets 865139255  bytes 892857486730 (892.8 GB)\n",
      "        RX errors 0  dropped 0  overruns 0  frame 0\n",
      "        TX packets 422954431  bytes 96300294274 (96.3 GB)\n",
      "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
      "        device memory 0x91580000-915fffff  \n",
      "\n",
      "ens121f1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\n",
      "        inet6 fe80::8261:5fff:fe22:4a04  prefixlen 64  scopeid 0x20<link>\n",
      "        ether 80:61:5f:22:4a:04  txqueuelen 1000  (Ethernet)\n",
      "        RX packets 3386375  bytes 376447569 (376.4 MB)\n",
      "        RX errors 0  dropped 0  overruns 0  frame 0\n",
      "        TX packets 2289  bytes 203466 (203.4 KB)\n",
      "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
      "        device memory 0x91480000-914fffff  \n",
      "\n",
      "ibs110: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 2044\n",
      "        inet 10.3.0.29  netmask 255.255.0.0  broadcast 10.3.255.255\n",
      "        inet6 fe80::ba3f:d203:38:b9a4  prefixlen 64  scopeid 0x20<link>\n",
      "        unspec 00-00-10-29-FE-80-00-00-00-00-00-00-00-00-00-00  txqueuelen 256  (UNSPEC)\n",
      "        RX packets 7478886378  bytes 9271082494133 (9.2 TB)\n",
      "        RX errors 0  dropped 0  overruns 0  frame 0\n",
      "        TX packets 5954267991  bytes 5637053394624 (5.6 TB)\n",
      "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
      "\n",
      "lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536\n",
      "        inet 127.0.0.1  netmask 255.0.0.0\n",
      "        inet6 ::1  prefixlen 128  scopeid 0x10<host>\n",
      "        loop  txqueuelen 1000  (Local Loopback)\n",
      "        RX packets 317280708  bytes 431944171551 (431.9 GB)\n",
      "        RX errors 0  dropped 0  overruns 0  frame 0\n",
      "        TX packets 317280708  bytes 431944171551 (431.9 GB)\n",
      "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
      "\n",
      "vxlan.calico: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1450\n",
      "        inet 10.234.54.0  netmask 255.255.255.255  broadcast 0.0.0.0\n",
      "        inet6 fe80::643d:bff:fe18:ba96  prefixlen 64  scopeid 0x20<link>\n",
      "        ether 66:3d:0b:18:ba:96  txqueuelen 1000  (Ethernet)\n",
      "        RX packets 26281174  bytes 25125351258 (25.1 GB)\n",
      "        RX errors 0  dropped 0  overruns 0  frame 0\n",
      "        TX packets 43432347  bytes 6012460162 (6.0 GB)\n",
      "        TX errors 0  dropped 3423 overruns 0  carrier 0  collisions 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the IP address\n",
    "\n",
    "!ifconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple pod multiple GPUs\n",
    "# run the following command in the terminal \n",
    "# (you need to set the rank to 0 or 1 for each pod, (master is 0), also set ehe msater IP addresss)\n",
    "# you should find the master_ip address by running ifconfig and look for the IP address starting with 10.3.x.x (200Gbps IB)\n",
    "\n",
    "# NCCL_P2P_DISABLE=1 NCCL_IB_DISABLE=1 torchrun --nnodes 2 --nproc_per_node 2 --node_rank 0 --master_addr 10.3.0.0 /tmp/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On the master node only, run the following in the terminal to start the tensorboard\n",
    "\n",
    "# tensorboard --logdir /scratch2/results/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Merge Lora_model with Base model and save the merged model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should first export Lora model with base model and convert them into hf checkpoint. \n",
    "This makes up the final trained merged model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Shared parameters between inference and SFT training\n",
    "################################################################################\n",
    "import torch\n",
    "\n",
    "# The base model\n",
    "model_name = \"/share/model/llama-2-7b-chat-hf\"\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,    # use 4-bit precision for base model loading\n",
    "    bnb_4bit_quant_type= \"nf4\",  # Quantization type (fp4 or nf4)\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,   # Compute dtype for 4-bit base models  \"float16\" or torch.bfloat16\n",
    "    bnb_4bit_use_double_quant= False,  # Activate nested quantization for 4-bit base models (double quantization)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f48ec012858493ca000177379e6e222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "new_model = \"/scratch2/llama-2-7b-py18k\"\n",
    "device_map = \"auto\"\n",
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Save merged model to disk (optional)\n",
    "merged_model.save_pretrained(f'{new_model}_merged')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 examine the results in tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up port forwarding in vscode\n",
    "# open the tensorboard page at http://localhost:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Test the SFTed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Write a Python function to return the mode (the value or values that appear most frequently within that list) in a given list. If there are multiple modes, return -1. You should generate a function:\n",
      " def solve(list): [/INST] ```def solve(list):\n",
      "    count = {}\n",
      "    for item in list:\n",
      "        count[item] = count.get(item, 0) + 1\n",
      "    max_count = max(count.values())\n",
      "    max_count_items = [item for item, count in count.items() if count == max_count]\n",
      "    if len(max_count_items) > 1:\n",
      "        return -1\n",
      "    else:\n",
      "        return max_count_items[0]\n",
      "\n",
      "list = [1, 2, 3, 3, 4, 5, 5, 5, 6, 6, 6, 7, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Write a Python function to return the mode (the value or values that appear most frequently within that list) in a given list. If there are multiple modes, return -1. You should generate a function:\\n def solve(list):\"\n",
    "pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer, temperature=0.3, do_sample=True, max_length=1024)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using regrex to capture the generated Python to a string\n",
    "import re\n",
    "\n",
    "def extract_first_code_snippet(text):\n",
    "    # Use a regular expression to find the first code snippet enclosed in triple backticks\n",
    "    match = re.search(r\"```(.*?)```\", text, re.S)\n",
    "    if match:\n",
    "        # Return the first matched group, which is the content within the backticks\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        # Return None if no match is found\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a testcase using the standard python unittest library\n",
    "\n",
    "import unittest\n",
    "\n",
    "# place holder for the AI generated code\n",
    "def solve(list):\n",
    "    return 0  \n",
    "\n",
    "class TestGeneratedCode(unittest.TestCase):\n",
    "\n",
    "    def test_no_single_mode(self):\n",
    "        self.assertEqual(solve([3, 2, 1]), -1)\n",
    "\n",
    "    def test_single_mode(self):\n",
    "        self.assertEqual(solve([4, 9, 2, 33, 2]), 2)\n",
    "\n",
    "    def test_no_single_mode_3(self):\n",
    "        self.assertEqual(solve([7, 9, 11, 323, 996]), -1)\n",
    "\n",
    "def run_all_tests():\n",
    "    unittest.main(argv=[''], verbosity=2, exit=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def solve(list):\n",
      "  # calculate the number of occurrences of each element in list\n",
      "  occurrences = collections.Counter(list)\n",
      "  # calculate the mode\n",
      "  mode = occurrences.most_common()[0][0]\n",
      "  # if there is only one mode\n",
      "  return mode if occurrences.most_common()[0][0] == 1 else -1\n",
      "\n",
      "# Test\n",
      "print(solve([1, 2, 3, 4, 2, 3, 4]))\n"
     ]
    }
   ],
   "source": [
    "code = extract_first_code_snippet(result[0]['generated_text'])\n",
    "print(code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m run_all_tests() \u001b[38;5;66;03m# Expect to pass\u001b[39;00m\n",
      "File \u001b[0;32m<string>:10\u001b[0m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36msolve\u001b[0;34m(list)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'collections' is not defined"
     ]
    }
   ],
   "source": [
    "exec(code)\n",
    "run_all_tests() # Expect to pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 (optional) Experience Zero-3 \n",
    "Now you can try DeepSpeed with a more aggressive strategy named `stage-3` or `zero-3` by replacing the DeepSpeed configuration file using `ds_zero3_offload_pt.json`. Rerun the training and observe the changes on GPU memory usage and training time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
