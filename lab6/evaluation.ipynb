{"cells": [{"cell_type": "markdown", "id": "324be893", "metadata": {}, "source": ["# Lab 6 Performance Evaluation"]}, {"cell_type": "markdown", "id": "d2b05339", "metadata": {}, "source": ["First, we need to install a few more dependencies. "]}, {"cell_type": "code", "execution_count": 1, "id": "91bb8bcf-229f-43ad-844f-f5114178be25", "metadata": {}, "outputs": [], "source": ["!pip3 install -r requirements.txt"]}, {"cell_type": "code", "execution_count": 2, "id": "0771d566", "metadata": {}, "outputs": [], "source": ["import os\n", "import requests\n", "import threading\n", "import evaluate\n", "import numpy as np\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": 3, "id": "7c90fbbe-a8e3-442e-b6ae-8591aa06a5f3", "metadata": {}, "outputs": [], "source": ["# add proxy to access huggingface ...\n", "os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""]}, {"cell_type": "markdown", "id": "fd59362b", "metadata": {}, "source": ["# 1. NLP related Metrics\n", "You can try to change the `prediction` and `reference` in the following sample code and see the range of the metrics."]}, {"cell_type": "markdown", "id": "59a76847-dfab-47cf-903b-3d427a41019c", "metadata": {}, "source": ["## 1.1 Accuracy\n", "Accuracy is the proportion of correct predictions among the total number of cases processed. \n", "\n", "It can be computed with: Accuracy = (TP + TN) / (TP + TN + FP + FN) Where: TP: True positive TN: True negative FP: False positive FN: False negative\n", "\n", "The metric ranges from 0 to 1 and a higher score is better."]}, {"cell_type": "code", "execution_count": 4, "id": "cafd11de", "metadata": {}, "outputs": [], "source": ["accuracy = evaluate.load('accuracy')"]}, {"cell_type": "code", "execution_count": 5, "id": "3d17ece6", "metadata": {}, "outputs": [], "source": ["accuracy.compute(references=[0,1,0,1], predictions=[0,1,0,0])"]}, {"cell_type": "code", "execution_count": 6, "id": "3546dbb9", "metadata": {}, "outputs": [], "source": ["accuracy.compute(references=[0,1,2,1,1], predictions=[1,2,2,1,1])"]}, {"cell_type": "markdown", "id": "f612208e", "metadata": {}, "source": ["## 1.2 BLEU\n", "BLEU, or the Bilingual Evaluation Understudy, is a score for comparing a candidate translation of text to one or more reference translations. The higher the BLEU score, the more similar the generated text is to the reference text. Its value ranges from 0 to 1.\n", "\n", "This metric compares the n-gram overlap between the machine translation result and the reference translation where an n-gram is a sequence of consecutive n words. \n", "\n", "The metric ranges from 0 to 1 and a higher score is better.\n"]}, {"cell_type": "markdown", "id": "911f7fd7-f85b-43bc-96f2-d40d7349608a", "metadata": {}, "source": ["### Single sentence score\n", "Observe how the score varies as the input changes."]}, {"cell_type": "code", "execution_count": 7, "id": "c447038e-bca2-4fee-ae43-6eb2d2221fd3", "metadata": {}, "outputs": [], "source": ["bleu = evaluate.load(\"bleu\")"]}, {"cell_type": "code", "execution_count": 8, "id": "3d2bc561", "metadata": {}, "outputs": [], "source": ["prediction1 = 'the cat is on the yoga mat'\n", "reference1 = 'the cat sat on the yoga mat'\n", "bleu_score = bleu.compute(predictions=[prediction1], references=[reference1])\n", "bleu_score['bleu']"]}, {"cell_type": "code", "execution_count": 35, "id": "1f3d5178", "metadata": {}, "outputs": [], "source": ["prediction2 = 'the value of life lies in what you create for others not in what you possess'\n", "reference2 = 'the meaning of life lies in what you give to others not in what you receive'\n", "bleu_score = bleu.compute(predictions=[prediction2], references=[reference2])\n", "bleu_score"]}, {"cell_type": "code", "execution_count": 10, "id": "1027d5c6", "metadata": {}, "outputs": [], "source": ["prediction3 = 'the adversary abusing Sybil accounts imposes a critical threat to establishing trust and integrity in web services'\n", "reference3 = 'establishing trust in web services is threatened by the adversary'\n", "bleu_score = bleu.compute(predictions=[prediction3], references=[reference3])\n", "bleu_score['bleu']"]}, {"cell_type": "markdown", "id": "6e575af7-86df-4c14-b87b-044b2faec668", "metadata": {}, "source": ["### Multiple sentence score"]}, {"cell_type": "code", "execution_count": 11, "id": "e2ea7b7a-f649-4b65-aae5-0c128bc087b6", "metadata": {}, "outputs": [], "source": ["bleu_score = bleu.compute(predictions=[prediction1, prediction2, prediction3], \n", "                          references=[reference1, reference2, reference3])\n", "bleu_score['bleu']"]}, {"cell_type": "markdown", "id": "6109f18a-8acd-42f2-9abc-15463736581d", "metadata": {}, "source": ["### Incremental adding predictions"]}, {"cell_type": "code", "execution_count": 12, "id": "109b6386-2f04-495f-8651-7c2b2966c311", "metadata": {}, "outputs": [], "source": ["bleu.add(predictions=prediction1, references=reference1)\n", "bleu.add(predictions=prediction2, references=reference2)\n", "bleu.add(predictions=prediction3, references=reference3)\n", "bleu_score = bleu.compute()\n", "bleu_score['bleu']"]}, {"cell_type": "markdown", "id": "ee4281fb", "metadata": {}, "source": ["## 1.3 ROUGE\n", "\n", "ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. \n", "\n", "The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation. Rouge-1 considers 1-gram, Rouge-2 2-gram and so on. Rouge-L considers the longest common subsequence.\n", "\n", "Note that ROUGE is case insensitive, meaning that upper case letters are treated the same way as lower case letters.\n", "\n", "The metric ranges from 0 to 1 and a higher score is better.\n"]}, {"cell_type": "code", "execution_count": 13, "id": "601bfbd1-f536-43df-872d-07471ed64bc1", "metadata": {}, "outputs": [], "source": ["rouge = evaluate.load(\"rouge\")"]}, {"cell_type": "code", "execution_count": 14, "id": "7e5b2dda", "metadata": {}, "outputs": [], "source": ["prediction1 = 'the cat is on the yoga mat'\n", "reference1 = 'the cat sat on the yoga mat'\n", "\n", "rouge_scores = rouge.compute(predictions=[prediction1], references=[reference1])\n", "rouge_scores"]}, {"cell_type": "code", "execution_count": 15, "id": "abe9028c", "metadata": {}, "outputs": [], "source": ["prediction2 = 'the value of life lies in what you create for others not in what you possess'\n", "reference2 = 'the meaning of life lies in what you give to others not in what you receive'\n", "\n", "rouge_scores = rouge.compute(predictions=[prediction2], references=[reference2])\n", "rouge_scores"]}, {"cell_type": "code", "execution_count": 16, "id": "6193791d", "metadata": {}, "outputs": [], "source": ["prediction3 = 'the adversary abusing Sybil accounts imposes a critical threat to establishing trust and integrity in web services'\n", "reference3 = 'establishing trust in web services is threatened by the adversary'\n", "\n", "rouge_scores = rouge.compute(predictions=[prediction3], references=[reference3])\n", "rouge_scores"]}, {"cell_type": "markdown", "id": "8324be0b-8e1b-4aec-87be-9dec5e071c59", "metadata": {}, "source": ["Rouge also supports multiple sentence score and incremental computing. You could try below if interested."]}, {"cell_type": "markdown", "id": "b6673456-2cd7-4562-ad40-f02bea867afd", "metadata": {}, "source": ["## 1.4 Perplexity\n", "\n", "Perplexity measures the uncertainty of a language model's predictions. \n", "\n", "Given a model and an input text sequence, perplexity measures how likely the model is to generate the input text sequence.\n", "\n", "The range of this metric is [0, inf). A lower score is better."]}, {"cell_type": "code", "execution_count": 17, "id": "b4f25a53-3709-423f-9dc5-3385477ebf03", "metadata": {}, "outputs": [], "source": ["perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")"]}, {"cell_type": "code", "execution_count": 18, "id": "c76fb396-ac1e-4f10-8024-de2ddabace7d", "metadata": {}, "outputs": [], "source": ["input_texts = [\"lorem ipsum\", \"Happy Birthday!\", \"Bienvenue\"]\n", "results = perplexity.compute(model_id='gpt2',\n", "                             add_start_token=False,\n", "                             predictions=input_texts) \n", "results"]}, {"cell_type": "markdown", "id": "375aaac6-df65-4be2-b773-fc0758cca44d", "metadata": {}, "source": ["## 1.5 Combining multiple metrics"]}, {"cell_type": "code", "execution_count": 19, "id": "0dfcce8f-1949-4ea8-bcbd-fefe28263655", "metadata": {}, "outputs": [], "source": ["metrics = evaluate.combine(['bleu', 'rouge'])\n", "scores = metrics.compute(predictions=[prediction1, prediction2, prediction3], \n", "                          references=[reference1, reference2, reference3])\n", "scores"]}, {"cell_type": "markdown", "id": "94ebdc82", "metadata": {}, "source": ["# 2 Evaluatings Models over REST API"]}, {"cell_type": "markdown", "id": "e2a3ad9c", "metadata": {}, "source": ["## 2.1 Some utility functions"]}, {"cell_type": "code", "execution_count": 20, "id": "ac89ad5e", "metadata": {}, "outputs": [], "source": ["get_res = requests.get('http://10.1.0.5:32411/auth/callback/debug?code=dev')\n", "access_token = eval(get_res.text)['access_token']['access_token']\n", "MAX_THREADS = 5"]}, {"cell_type": "code", "execution_count": 21, "id": "7db095da", "metadata": {}, "outputs": [], "source": ["# Invoke the large language model API.\n", "def get_llm_output(model_name, access_token, question_content, question_id, output, semaphore):\n", "    try:\n", "        print(f\"starting question %s\" % (question_id))\n", "        while True:\n", "            try:\n", "                post_res = requests.post('http://10.1.0.5:32411/v1/chat/completions',\n", "                                    headers={'Content-Type': 'application/json',\n", "                                            'Authorization': 'Bearer '+access_token},\n", "                                    json={\"model\": model_name, \"stream\": False,\n", "                                            \"messages\": [{\"content\": question_content, \"role\": \"user\"}]})\n", "                if post_res.ok: break\n", "            except requests.exceptions.RequestException as e: print(f\"An error occurred: {e}\") \n", "        llm_answer = eval(post_res.text)\n", "        if 'data' not in llm_answer:\n", "            raise Exception(\"The connection to LLM API seems disconnected. It returns: %s\"%(llm_answer))\n", "        llm_answer = llm_answer['data']['choices'][0]['message']['content']\n", "        llm_answer = llm_answer.replace(\"<|im_end|>\",\"\").replace(\"<|im_start|>\",\"\")\n", "        output[question_id] = llm_answer\n", "        print(f\"answered question %s\" % (question_id))\n", "    finally:\n", "        semaphore.release()"]}, {"cell_type": "code", "execution_count": 22, "id": "4d519267-d72c-4dfb-b0f6-ba4f52608dc1", "metadata": {}, "outputs": [], "source": ["def get_llm_output_parallel(model_name, access_token, question_contents, max_threads=5):\n", "    # Create threads for each question\n", "    output = {}\n", "    threads = []\n", "    semaphore = threading.Semaphore(max_threads)\n", "    for question_id, question_content in enumerate(question_contents):\n", "        semaphore.acquire() \n", "        thread = threading.Thread(target=get_llm_output, args=(model_name, access_token, question_content, question_id, output, semaphore))\n", "        threads.append(thread)\n", "        thread.start()\n", "\n", "    # Wait for all threads to complete\n", "    for thread in threads:\n", "        thread.join()\n", "\n", "    sorted_keys = sorted(output.keys())\n", "    sorted_outputs = [output[key] for key in sorted_keys]        \n", "    return sorted_outputs"]}, {"cell_type": "code", "execution_count": 23, "id": "c333379c-d4cb-4fdd-94ef-209bcddffa97", "metadata": {}, "outputs": [], "source": ["def print_llm_outputs(model_name, question_contents, llm_answers, references):\n", "    for i, (question, answer, reference) in enumerate(zip(question_contents, llm_answers, references)):\n", "        print('Question %d: %s'%(i, question))\n", "        print('Answer from Model %s: %s'%(model_name, answer))\n", "        print('Reference Answer: %s\\n'%(reference))"]}, {"cell_type": "markdown", "id": "27002a74-55ee-48df-9850-845265ac95c1", "metadata": {}, "source": ["## 2.2 Example: Evaluate News Summarization Results"]}, {"cell_type": "markdown", "id": "54000bb7-0726-4c8c-b5de-2f5c074ff4d9", "metadata": {}, "source": ["### Load the data and preprocess\n", "\n", "The CNN / DailyMail Dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. \n", "\n", "For each instance, there is a string for the article, a string for the highlights, and a string for the id.\n", "\n", "Data source: https://huggingface.co/datasets/cnn_dailymail"]}, {"cell_type": "code", "execution_count": 24, "id": "60884967-ad63-49d1-9a03-8df35dac5c21", "metadata": {}, "outputs": [], "source": ["from datasets import load_dataset, load_from_disk\n", "\n", "# d=load_dataset(r\"ccdv/cnn_dailymail\", '3.0.0')\n", "# d.save_to_disk('/share/data/cnn_dailymail/')\n", "d = load_from_disk('/share/data/cnn_dailymail/')"]}, {"cell_type": "code", "execution_count": 25, "id": "bb1b9395-6e17-447f-9cf8-ef54c65a5b65", "metadata": {}, "outputs": [], "source": ["def create_prompt(x):\n", "    s = \"Please summarize the following news article in no more than 30 words.\\n %s\" %(x['article'])\n", "    x['question_content'] = s\n", "    return x\n", "\n", "d = d['test'].map(create_prompt) # Use test set to evaluate\n", "d"]}, {"cell_type": "markdown", "id": "deffa166-871b-4e14-b931-e82840318fab", "metadata": {}, "source": ["### Evaluate the models\n", "It is a summarization task, so we can use both BLEU and ROUGE as evaluation metrics."]}, {"cell_type": "code", "execution_count": 26, "id": "69feb89f-93e6-4746-9c5d-020793d09ab4", "metadata": {"scrolled": true}, "outputs": [], "source": ["EVALUATE_N= 10 # To save time, we evaluate 10 articles.\n", "metrics = evaluate.combine(['bleu', 'rouge'])\n", "overall_scores = {}\n", "\n", "for model_name in ['Qwen-7b-chat', 'Llama-2-7B-Chat-fp16',]:\n", "    print(f'============== {model_name}  ==============')\n", "    question_contents = d['question_content'][:EVALUATE_N]\n", "    references = d['highlights'][:EVALUATE_N]\n", "    llm_answers = get_llm_output_parallel(model_name, access_token, question_contents, max_threads=MAX_THREADS)\n", "    scores = metrics.compute(predictions=llm_answers, references=references)\n", "    overall_scores[model_name] = [scores['bleu'], scores['rouge1'], scores['rouge2'], scores['rougeL']]    \n", "    print_llm_outputs(model_name, question_contents, llm_answers, references)"]}, {"cell_type": "code", "execution_count": 27, "id": "8f1cb464-eb88-423f-80e0-6eb8d4f88f9b", "metadata": {}, "outputs": [], "source": ["# nice print the results using pandas\n", "import pandas as pd\n", "performance_df = pd.DataFrame(overall_scores)\n", "performance_df.index = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n", "performance_df"]}, {"cell_type": "code", "execution_count": 28, "id": "4d89b7aa-b410-41c2-814c-c51c56e77e4f", "metadata": {}, "outputs": [], "source": ["performance_df.plot.bar()\n", "plt.ylabel('Evaluation metric score')\n", "plt.title('LLM performance in news summarization')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "ef9ff7cf", "metadata": {}, "source": ["## 2.3 Multiple choice question answering"]}, {"cell_type": "markdown", "id": "074d8a57-06db-493b-9377-5979bc2865e9", "metadata": {}, "source": ["### Load the data from huggingface\n", "C-Eval is a comprehensive Chinese evaluation suite for foundation models. It consists of 13948 multi-choice questions spanning 52 diverse disciplines and four difficulty levels. Here we use two disciplines: art studies and operating system.\n", "\n", "Data source: https://cevalbenchmark.com/index_zh.html"]}, {"cell_type": "code", "execution_count": 29, "id": "8103106e", "metadata": {}, "outputs": [], "source": ["def get_question_content(x):\n", "    s = \"\u4ee5\u4e0b\u662f\u5355\u9879\u9009\u62e9\u9898\uff0c\u8bf7\u76f4\u63a5\u7ed9\u51fa\u5176\u4e2d\u7684\u6b63\u786e\u7b54\u6848\u3002\u8bf7\u53ea\u8f93\u51faABCD\u5f53\u4e2d\u7684\u4e00\u4e2a\uff0c\u4e0d\u9700\u8981\u4f5c\u89e3\u91ca\u3002\\n%s\\nA. %s\\nB. %s\\nC. %s\\nD. %s\" %(x['question'],x['A'], x['B'], x['C'], x['D'])\n", "    x['question_content'] = s\n", "    return x"]}, {"cell_type": "code", "execution_count": 30, "id": "488f1ece-ca2b-4e0b-9f3c-5559dbcbec4f", "metadata": {}, "outputs": [], "source": ["data_names = ['art_studies', 'operating_system']\n", "ds = []\n", "\n", "for data_name in data_names:\n", "    d=load_dataset(r\"ceval/ceval-exam\", name=data_name)\n", "    d_updated = d['val'].map(get_question_content) # Use validation set to evaluate\n", "    print(data_name)\n", "    print(d_updated)\n", "    ds.append(d_updated)"]}, {"cell_type": "markdown", "id": "da2d6db1-f419-4212-affd-4f33f69f92ff", "metadata": {}, "source": ["### Evaluate the models\n", "Accuracy is used to evaluate the model."]}, {"cell_type": "code", "execution_count": 31, "id": "250b569d-1598-4d07-b0d6-d1a1b70e3a39", "metadata": {}, "outputs": [], "source": ["def get_options(llm_answers):\n", "    # Select the option that occurs most times in the model output as the final answer.\n", "    options = []\n", "    for llm_answer in llm_answers:\n", "        option_frequencies = [llm_answer.count(option) for option in 'ABCD']\n", "        most_frequent = np.argmax(option_frequencies)\n", "        most_frequent_option = 'ABCD'[most_frequent]\n", "        options.append(most_frequent_option)\n", "    return options\n", "\n", "def option2num(options):\n", "    # Transform the ABCD options to numbers for accuracy evaluation.\n", "    option2num_dict = {'A':0 ,'B':1, 'C':2, 'D':3}\n", "    nums = list(map(lambda x:option2num_dict[x], options))\n", "    return nums"]}, {"cell_type": "code", "execution_count": 32, "id": "a3f65cb3-1220-41a6-bf6c-e1a3916cf145", "metadata": {"scrolled": true}, "outputs": [], "source": ["overall_scores = {} # Evaluation results for all models\n", "\n", "for model_name in ['Qwen-7b-chat',  'Llama-2-7B-Chat-fp16',]:\n", "    scores = []\n", "    print(f'============== {model_name}  ==============')\n", "    for i, d in enumerate(ds):\n", "        print('Data %s has %d questions'%(data_names[i], d.num_rows))\n", "        question_contents = d['question_content']\n", "        llm_answers = get_llm_output_parallel(model_name, access_token, question_contents, max_threads=MAX_THREADS)\n", "        print_llm_outputs(model_name, question_contents, llm_answers, d['answer'])        \n", "        llm_answers = get_options(llm_answers)\n", "        acc = accuracy.compute(references=option2num(d['answer']), predictions=option2num(llm_answers)) \n", "        scores.append(acc['accuracy'])\n", "    overall_scores[model_name] = scores"]}, {"cell_type": "code", "execution_count": 33, "id": "57278fe2", "metadata": {}, "outputs": [], "source": ["accuracy_df =  pd.DataFrame(overall_scores)\n", "accuracy_df.index = data_names\n", "accuracy_df"]}, {"cell_type": "code", "execution_count": 34, "id": "86077a04", "metadata": {}, "outputs": [], "source": ["accuracy_df.plot.barh()\n", "plt.xlabel('Accuracy')\n", "plt.title('LLM performance in C-Eval benchmark')\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.13"}}, "nbformat": 4, "nbformat_minor": 5}