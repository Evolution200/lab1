{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## 3 Evaluating Locally deployed models"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.1 Load the (Quantized) model to a single GPU"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import accelerate, bitsandbytes\n", "import torch, os\n", "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n", "\n", "from transformers import LlamaTokenizerFast\n", "\n", "model_path = '/share/model/llama-2-7b-chat-hf/'\n", "# model_path = '/ssdshare/LLMs/llama3-Chinese-chat-8b/'\n", "tokenizer = LlamaTokenizerFast.from_pretrained(model_path,padding_side='left')\n", "qconfig=BitsAndBytesConfig(load_in_8bit=True)\n", "\n", "model = AutoModelForCausalLM.from_pretrained(model_path, \n", "                                             device_map=\"cuda:0\", \n", "                                             quantization_config=qconfig) \n", "tokenizer.pad_token = tokenizer.eos_token\n", "tokenizer.pad_token_id = tokenizer.eos_token_id"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Verify that the model is loaded to GPU (look at the memory utilization)."]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["!nvidia-smi"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.2 Generate responses locally"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["def chat_resp(model, tokenizer, question_list):\n", "    # question_list is a list of questions\n", "    inputs = tokenizer(question_list, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096).input_ids.to(\"cuda\")\n", "    outputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=512, do_sample=True, temperature=0.7)\n", "    resp = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n", "    return resp\n", "\n", "def chat_resp_batched(model, tokenizer, question_list, batch_size=4):\n", "    # Split the question list into batches of the specified size\n", "    batches = [question_list[i:i + batch_size] for i in range(0, len(question_list), batch_size)]\n", "    all_responses = []\n", "    \n", "    for batch in batches:\n", "        print(f\"processing batch: %s \" % batch)\n", "        responses = chat_resp(model, tokenizer, batch)\n", "        all_responses.extend(responses)\n", "    return all_responses"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["def gsm8k_prompt(question):\n", "    chat = [\n", "        {\"role\": \"system\", \"content\": \"\"\"Please solve the given math problem by providing a detailed, step-by-step explanation. Begin by outlining each step involved in your solution, ensuring clarity and precision in your calculations. After you have worked through the problem, conclude your response by summarizing the solution and stating the final answer as a single exact numerical value on the last line. \"\"\"},\n", "        {\"role\": \"user\", \"content\": \"Question: \" + question},\n", "    ]\n", "\n", "    s = tokenizer.apply_chat_template(chat, tokenize=False)\n", "\n", "    return s"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["## Test the model with a sample question\n", "\n", "p = gsm8k_prompt(\"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\")\n", "p = [p]\n", "resp = chat_resp(model, tokenizer, p)\n", "print(resp[0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.3 Prepare the evaluation datasets"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["# add proxy to access huggingface ...\n", "os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["from datasets import load_dataset\n", "dataset = load_dataset(\"gsm8k\", \"main\")\n", "\n", "# to save time, we only use a small subset\n", "subset = dataset['test'][5:30]\n", "questions = subset['question']\n", "answers = subset['answer']\n", "\n", "dataset"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["# We only want the numeric answers from the dataset for evalation (maybe a bad choice?)\n", "\n", "def get_exact_answer(x):\n", "    i = x.index('####')\n", "    return x[i+5:].strip('\\n')\n", "\n", "num_answers = list(map(get_exact_answer, answers))\n", "print(num_answers)\n"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["# this is very tentative and bad way to find the exact answer, consider fixing it. \n", "\n", "import re\n", "def get_numbers(s):\n", "    number =[]\n", "    lines = s.split('\\n')\n", "    for i in range(-1, -len(lines), -1):\n", "        number = re.findall(r'\\d+(?:\\.\\d+)?', lines[i])\n", "        if len(number) > 0:\n", "            break\n", "    if (len(number) == 0):\n", "        return '-9999'\n", "    return number[-1]  # the last number is the answer"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["t = \"\"\"\n", "Toulouse has twice as many sheep as Charleston. Charleston has 4 times as many sheep as Seattle. How many sheep do Toulouse, Charleston, and Seattle have together if Seattle has 20 sheep?\n", "\n", "Solution:\n", "Let's start by using the information we know:\n", "\n", "Toulouse has twice as many sheep as Charleston, so Toulouse has 2x = 2 \\* 4 = 8 sheep.\n", "Charleston has 4 times as many sheep as Seattle, so Charleston has 4 \\* 20 = 80 sheep.\n", "So, Toulouse has 8 sheep, Charleston has 80 sheep, and Seattle has 20 sheep.\n", "Together, they have 8 + 80 + 20 = 128 sheep.\n", "\n", "\n", "\"\"\"\n", "\n", "get_numbers(t)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.4 Evaluate!"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["question_prompts = [gsm8k_prompt(q) for q in questions]\n", "resps = chat_resp_batched(model, tokenizer, question_prompts, batch_size=5)\n", "\n", "llm_answers = []\n", "\n", "for resp in resps:\n", "    print(\"--------\")\n", "    print(resp)\n", "    print(\"--------\")\n", "    num = get_numbers(resp)\n", "    print(num)\n", "    llm_answers.append(num)\n", "    print(\"---------\" )\n", "    print(llm_answers)"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["print(llm_answers)\n", "print(num_answers)"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["## manual way to compute the correct rate\n", "\n", "error = 0\n", "for i in range(0, len(llm_answers)):\n", "    if llm_answers[i] != num_answers[i]:\n", "        error += 1\n", "print(f\"number of errors: %s \\n correct rate: %s\" % (error, 1 - error / len(llm_answers))) "]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["## the way of using HuggingFace evaluate functions\n", "\n", "import evaluate\n", "exact_match = evaluate.load(\"exact_match\")\n", "results = exact_match.compute(predictions=llm_answers, references=num_answers)\n", "print(results)"]}], "metadata": {"kernelspec": {"display_name": "base", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.13"}}, "nbformat": 4, "nbformat_minor": 2}