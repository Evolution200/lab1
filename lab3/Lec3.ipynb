{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Lec3. LangChain\n", "\n", "\n", "## Introduction\n", "\n", "\n", "**LangChain** is a framework for developing applications powered by language models. It provides abundant abstractions about langage models and sources of context (prompt instructions, few shot examples, content to ground its response in, etc.), which enable the user to easily **chain** these components together for developing awesome applications.\n", "\n", "In this lab, we will learn several key abstractions in LangChain and build an input-output customized AI-powered web-search application.\n", "\n", "### Reference \n", "1. [Langchain document](https://python.langchain.com/docs/get_started/quickstart)\n", "\n", "\n", "## 0. First thing first\n", "\n", "### 0.1 Dependencies and Keys\n", "  \n", "You willl need at least two keys for the lab.  Please put them in the .env file.\n", "- OpenAI api key:\n", "    ```\n", "    OPENAI_API_KEY=\"sk-YOURKEY\"\n", "    ```\n", "- Serp api key:\n", "    ```\n", "    SERP_API_KEY=\"YOURKEY\"\n", "    ```\n", "    The `SERP_API_KEY` is for invoking the search engine, first register through this [web site](https://serpapi.com/).\n", "\n", "    After getting these two keys, set your keys as environment variables.\n", "- Langchain API key (for tracing)\n", "    ```\n", "    LANGCHAIN_TRACING_V2=\"true\"\n", "    LANGCHAIN_API_KEY=ls_xxxxxxxx\n", "    ```\n", "    "]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["# We have installed these dependencies in your image\n", "#%pip install -r requirements.txt"]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": ["from dotenv import load_dotenv  \n", "import os  \n", "\n", "load_dotenv()\n", "# OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') \n", "# SERPAPI_API_KEY = os.getenv('SERPAPI_API_KEY')"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["import os\n", "os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": ["MODEL = \"gpt-3.5-turbo-instruct\"\n", "CHAT_MODEL=\"gpt-3.5-turbo\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Key abstractions in LangChain\n", "\n", "| Abstracted Components | Input Type                                | Output Type           |\n", "|-----------------------|-------------------------------------------|-----------------------|\n", "| Prompt                | Dictionary                                | PromptValue           |\n", "| LLM                   | string, list of messages or a PromptValue | string, message       |\n", "| ChatModel             | string, list of messages or a PromptValue | string, ChatMessage   |\n", "| OutputParser          | The output of an LLM or ChatModel         | Depends on the parser |"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.1 LLM and ChatModel\n", "\n", "The language model is the core of LangChain, which contains two types: \n", "\n", "- `llms`: this is a language model which takes a string as input and returns a string.\n", "- `ChatModels`: this is a language model which takes a list of messages or a string as input and returns a message or a string.\n", "\n", "Both `llm` and `ChatModel` provides two methods to interact with the user:\n", "\n", "- `predict`: takes in a string, returns a string.\n", "- `predict_messages`: takes in a list of messages, returns a message.\n", "\n", "The most significant difference between normal LLM model and ChatModel is that the ChatModel is fintuned for chatting situation, while normal LLM model is to simply fillup your sentence.\n"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["# some output utilities \n", "def print_with_type(res):\n", "    print(f\"%s : %s\" % (type(res), res))\n"]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": ["from langchain_openai import OpenAI\n", "\n", "# LLM model\n", "\n", "llm = OpenAI(temperature=0, model=MODEL)\n", "qtext = \"hello! my name is xu wei, nice to meet you! could you tell me something about large language models\"\n", "res = llm.invoke(qtext)\n", "print_with_type(res) # llm simply fulfills the qtext."]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["# ChatModel\n", "from langchain_openai import ChatOpenAI\n", "from langchain.schema import HumanMessage\n", "\n", "chat_model = ChatOpenAI(temperature=0, model=CHAT_MODEL)\n", "qtext = \"hello! my name is xuwei, nice to meet you! could you tell me something about langchain\"\n", "\n", "messages = []\n", "messages.append(HumanMessage(content=qtext))\n", "res = chat_model.invoke(messages)\n", "\n", "print_with_type(res)\n", "\n", "messages.append(res)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The constructors are tedious to use, and you can use the following more friendly API. "]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["# a simpler way to manage messages\n", "from langchain.memory import ChatMessageHistory\n", "history = ChatMessageHistory()\n", "\n", "history.add_user_message(\"hi!\")\n", "history.add_ai_message(\"whats up?\")\n", "history.add_user_message(\"nothing much, you?\")\n", "\n", "res = chat_model.invoke(history.messages)\n", "print_with_type(res)\n"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["# remembering the chat history and context\n", "\n", "qtext = \"what is its application?\"\n", "messages.append(HumanMessage(content=qtext))  ## providing context of chat histroy\n", "res = chat_model.invoke(messages)\n", "print_with_type(res)\n", "messages.append(res)  ## remembers the histroy"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.2 Prompt templates\n", "\n", "LangChain provides PromptTemplate to help formatting the prompts.\n", "\n", "The most plain prompt is in the type of a ``string``. Usually, the prompt includes several different type of `Messages`, which contains the `role` and the plain prompt as `content`.\n", "\n", "There are four roles in LangChain, and you can define your own custom roles.\n", "\n", "- `HumanMessage`: A ChatMessage coming from a human/user.\n", "- `AIMessage`: A ChatMessage coming from an AI/assistant.\n", "- `SystemMessage`: A ChatMessage coming from the system.\n", "- `FunctionMessage`: A ChatMessage coming from a function call."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Simple template"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["# Prompt Template\n", "from langchain.prompts import PromptTemplate\n", "\n", "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n", "input_prompt = prompt.format(product=\"candies\")\n", "\n", "print_with_type(input_prompt)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Chat prompt template"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["# Chat Template (a list of temlates in a chat prompt template)\n", "\n", "from langchain.prompts.chat import ChatPromptTemplate\n", "\n", "# format chat message prompt\n", "sys_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n", "human_template = \"{text}\"\n", "\n", "chat_prompt = ChatPromptTemplate.from_messages([\n", "    (\"system\", sys_template),\n", "    (\"human\", human_template),\n", "])\n", "chat_input = chat_prompt.format_messages(input_language=\"English\", output_language=\"Chinese\", text=\"I love programming.\")\n", "\n", "print_with_type(chat_input)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Using template in the chat model"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["# format messages with PromptTemplate with translator as an example\n", "\n", "chat_input = chat_prompt.format_messages(input_language=\"English\", output_language=\"Chinese\", text=qtext)\n", "print_with_type(chat_input)\n", "print_with_type(chat_model.invoke(chat_input))\n", "\n", "messages = chat_input + messages  ## the system message must be at the beginning\n", "print_with_type(messages)\n", "\n", "res = chat_model.invoke(messages)\n", "print_with_type(res)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.3 Chaining Components together\n", "\n", "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components. \n", "In LangChain, most of the above key abstraction components are `Runnable` objects, and we can **chain** them together to build awesome applications. \n", "\n", "LangChain makes the chainning powerful through **LangChain Expression Language (LCEL)**, which can support chainning in manners of:\n", "\n", "- Async, Batch, and Streaming Support: any chain constructed in LCEL can automatically have full synv, async, batch and streaming support. \n", "- Fallbacks: due to many factors like network connection or non-deterministic properties, your LLM applications need to handle errors gracefully. With LCEL, your can easily attach fallbacks any chain.\n", "- Parallelism: since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n", "- LangSmith Tracing Integration: (for debugging, see below).\n", "\n", "In lab class, we only demonstrate the simplest functional chainning."]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["# More abstractions: bundling prompt and the chat_model into a chain\n", "\n", "translate_chain = chat_prompt | chat_model\n", "qtext = \"this is input to a chain of chat model and chat prompt.\"\n", "translate_chain.invoke({\n", "    \"input_language\": \"English\", \n", "    \"output_language\": \"Chinese\", \n", "    \"text\": {qtext}\n", "    })"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.4 Output parser\n", "\n", "Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.\n", "Langchain provides several commonly-used output parsers like [list parser](https://python.langchain.com/docs/modules/model_io/output_parsers/comma_separated), [datetime parser](https://python.langchain.com/docs/modules/model_io/output_parsers/datetime) and [enum parser](https://python.langchain.com/docs/modules/model_io/output_parsers/enum)."]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["# a simple parser\n", "# StdOutParser converts the chat message to a string.\n", "\n", "from langchain_core.output_parsers import StrOutputParser\n", "output_parser = StrOutputParser()\n", "\n", "stdoutchain = chat_prompt | chat_model | output_parser\n", "\n", "qtext = \"this is input to a chain of chat model and chat prompt.\"\n", "stdoutchain.invoke({\n", "    \"input_language\": \"English\", \n", "    \"output_language\": \"Chinese\", \n", "    \"text\": {qtext}\n", "    })"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### From Results to a Python Object\n", "Here we demonstrate a more powerful [pydantic parser](https://python.langchain.com/docs/modules/model_io/output_parsers/pydantic) as an example."]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": ["from typing import List\n", "from langchain.output_parsers import PydanticOutputParser\n", "from langchain.pydantic_v1 import BaseModel, Field\n", "\n", "class Professor(BaseModel):\n", "    name: str = Field(description=\"name of the Professor\")\n", "    publication_list: List[str] = Field(description=\"the list of the professor's publications.\")\n", "\n", "parser = PydanticOutputParser(pydantic_object=Professor)\n", "\n", "prompt = PromptTemplate(\n", "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\",\n", "    input_variables=[\"query\"],\n", "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n", ")\n", "\n", "professor_chain = prompt | llm | parser\n", "query = \"tell me about professor Wei Xu.\"\n", "output = professor_chain.invoke({\n", "    \"query\": {query}\n", "    })\n", "print_with_type(output)\n"]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": ["# Using the chat model\n", "\n", "professor_chat_chain = prompt | chat_model | parser\n", "output = professor_chat_chain.invoke({\n", "    \"query\": {query}\n", "    })\n", "print_with_type(output)"]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "# see how langchain organizes the input to construct the result.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You will see that the paper list does not contain much of information and lots of hallucination.  We continue to show how we can eliminate these problems."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 2. Adding more contexts"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.1 Retrievers\n", "\n", "Many LLM applications require user-specific data that is not part of the model's training set, like the above example : )\n", "The primary way of accomplishing this is through **Retrieval Augmented Generation (RAG)**. In this process, external data is retrieved and then passed to the LLM when doing the generation step. `Retriever` is an interface that returns documents given an unstructured query, which is used to provide the related contents to LLMs\n", "\n", "LangChain provides all the building blocks for RAG applications - from simple to complex, including document loaders, text embedding models and web searches.  We will introduce these models in Lab 4.  Here, we only use two very basic retrievers that does web search and local file access.  \n", "\n", "- web search: https://python.langchain.com/docs/modules/data_connection/retrievers/web_research\n", "local file: https://python.langchain.com/docs/modules/data_connection/document_loaders/ "]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["# Using the search API\n", "\n", "from langchain.utilities import SerpAPIWrapper\n", "\n", "search = SerpAPIWrapper()\n", "results = search.run(\"Nvidia\")\n", "print_with_type(results)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's put the search and LLM together."]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["from langchain.schema.runnable import RunnablePassthrough\n", "\n", "class News(BaseModel):\n", "    title: List[str] = Field(description=\"title list of the news\")\n", "    brief_desc: List[str] = Field(description=\"brief descrption of the corresponding news\")\n", "\n", "parser = PydanticOutputParser(pydantic_object=News)\n", "\n", "prompt = PromptTemplate(\n", "    template=\"Answer the user query based on the following context: \\n{context}\\n{format_instructions}\\nQuery: {query}\",\n", "    input_variables=[\"query\"],\n", "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n", ")\n", "\n", "llm.temperature = 0\n", "\n", "search = SerpAPIWrapper()\n", "setup_and_retrieval = {\n", "        \"context\": search.run,  # passing a retriever\n", "        \"query\": RunnablePassthrough()\n", "}\n", "websearch_chain = setup_and_retrieval | prompt | llm | parser\n", "\n", "res = websearch_chain.invoke(\"tell me about the following companies: nvidia, AMD, google and microsoft, and write a brief summary for each\")\n", "\n", "print_with_type(res)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.2 Debugging and Logging"]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": ["# Debugging and logging: verbose mode\n", "from langchain.globals import set_verbose\n", "set_verbose(True)\n", "\n", "# Try rerun the previous example to see the verbose output.\n"]}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [], "source": ["set_verbose(False)"]}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [], "source": ["# Debugging and logging: debug mode\n", "from langchain.globals import set_debug\n", "set_debug(True)\n", "\n", "# Try rerun the previous example to see the verbose output."]}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [], "source": ["set_debug(False)"]}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [], "source": ["# Debugging and logging: tracing \n", "# Add LANGCHAIN_TRACING_V2=\"true\" in your environment (.env)\n", "# Also make sure that you have LANGCHAIN_API_KEY set in your environment\n", "\n", "# Try rerun the previous example and goto https://smith.langchain.com/ to see the traces. "]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "# retrieve the information and fix the query results about Prof. Xu, generating the correct Professor object.\n", "# Note that you do not have to get a perfect answer from the LLM in this lab.  (if the answer is not perfect, please analyze and debug it in the next cell.)\n"]}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "# analyze the answer, if the answer is not correct, write down some comments about starting from which point, the answers start to be wrong. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 3. Smarter workflow: Agents\n", "\n", "In ``Chains``, a sequence of actions is hardcoded (in code). While in ``Agent``s, a language model is used as a reasoning engine to determine which actions to take and in which order.\n", "\n", "The key components of an ``Agent`` includes:\n", "\n", "1. Tools: Descriptions of available tools for the agent to call, which includes two key components: \n", "\n", "    - callable function: the right access for the agenet and \n", "    - description: giving the agent the clue for which tool to use.\n", "\n", "\n", "2. User input: The high level objective.\n", "\n", "3. Intermediate steps: Any (action, tool output) pairs previously executed in order to achieve the user input\n", "\n", "Also, LangChain has provided several [different types of agents](https://python.langchain.com/docs/modules/agents/agent_types/), and in this class, we show the simplest and the most common one, the [ReAct Agent](https://arxiv.org/pdf/2210.03629.pdf).\n", "\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Letter couting example"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Try the following very simple example, and see if LLM can get it correct."]}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [], "source": ["llm.invoke(\"how many letters in sentence \u2018i love yao class? without counting space\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's fix the above problem using Agent.  Agent can use tools, let's first create a  very simple tool.\n", "\n", "* Note that the comments in the tools are very important in developing AI tools. They are NOT optional! *"]}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [], "source": ["from langchain.agents import tool\n", "\n", "@tool\n", "def get_sentence_length(sentence: str) -> int:\n", "    \"\"\"Returns the length of the input.\"\"\"\n", "    return sum(c.isalpha() for c in sentence)\n", "\n", "tools = [ get_sentence_length ]\n", "\n", "print(tools)"]}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [], "source": ["from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n", "\n", "prompt = ChatPromptTemplate.from_messages(\n", "    [\n", "        ( \"system\", \"You are very powerful assistant who can use tools, but bad at calculating lengths of sentences.\", \n", "         ),\n", "        (\"user\", \"{input}\"),\n", "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # used to store the previous agent tool invocations and the corresponding tool outputs. \n", "    ]\n", ")"]}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [], "source": ["from langchain.agents import initialize_agent\n", "\n", "agent_chain = initialize_agent(tools, \n", "                               llm, \n", "                               agent=\"zero-shot-react-description\", \n", "                               prompt_template=prompt, \n", "                               verbose=False\n", "                               )\n", "\n", "agent_chain.invoke({\"input\": \"how many letters in sentence \u2018i love yao class'? without counting space\"})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Your Task: Create an auto-web-search AI Agent\n", "\n", "In this exercise, you are required to implement a web-search ai agent, which can search for anything you asked and it should return a summary with less than 100 words for you."]}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [], "source": ["from langchain.agents import load_tools #, create_react_agent, AgentExecutor\n", "\n", "parser = PydanticOutputParser(pydantic_object=News)\n", "prompt = ChatPromptTemplate.from_messages(\n", "    [\n", "        (\n", "            \"system\",\n", "            \"You are very powerful assistant, helping the users search the web and write summary for the user's interested topic: {keyword}\",\n", "        ),\n", "        (\"user\", \"{keyword}\"),\n", "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # used to store the previous agent tool invocations and the corresponding tool outputs. \n", "    ]\n", ")\n", "\n", "# @tool\n", "# def summary_length_checker(summary: str) -> bool:\n", "#     \"\"\"check whether the summary satisfies the length requirement, which should be less than 100 words, if it is false, please write a shorter summary.\n", "#     \"\"\"\n", "#     words = summary.split()  \n", "#     word_count = len(words) \n", "#     return word_count < 100\n", "\n", "tools = [load_tools([\"serpapi\"], llm)[0]]\n", "\n", "agent_chain = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", prompt_template=prompt, verbose=True)"]}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [], "source": ["agent_chain.invoke(\"tell me the news from tsinghua university within last week?\")"]}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "# use agent to find about prof. wei xu and his publication list.  and compare the results with the previous results.  better or worse?\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.2 Using the langchian hub"]}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [], "source": ["# AI-Powered web search application\n", "\n", "\n", "from langchain_openai import OpenAI\n", "from langchain import hub\n", "from langchain.agents import load_tools, create_react_agent, AgentExecutor\n", "\n", "search_query = \"What is the whether of today's Beijing?  give the temperature in celcius.\"\n", "\n", "llm=OpenAI(temperature=0, verbose=True, model=MODEL)\n", "tools = load_tools([\"serpapi\"], llm)\n", "\n", "prompt = hub.pull(\"hwchase17/react\")\n", "agent = create_react_agent(llm, tools, prompt)\n", "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n", "\n", "agent_executor.invoke({\"input\": search_query})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.3 Explore built-in tools"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Langchain has provided a collection of very interesting tools.  For example, we can use the wikipedia tool to find out what is Prof. Yao's most significant scientific contribution in computer science.  \n", "\n", "You can read more about the tools documentation at https://python.langchain.com/docs/modules/agents/tools/  .  The key apis are \n", "\n", "- tool.name\n", "- tool.description\n", "- tool.args\n", "\n", "You can find a list of useful tools on this page.\n", "https://python.langchain.com/docs/integrations/tools/ "]}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [], "source": ["%pip install  wikipedia"]}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [], "source": ["from langchain.tools import WikipediaQueryRun\n", "from langchain_community.utilities import WikipediaAPIWrapper\n", "\n", "\n", "\n", "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n", "print_with_type(wikipedia.run(\"andrew yao\"))\n", "\n", "print(wikipedia.name)  # the tool name\n", "\n"]}, {"cell_type": "code", "execution_count": 44, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "# use the wikipedia tool to write a summary about the main scientific contribution of Andrew Yao, the computer scientist.\n"]}, {"cell_type": "code", "execution_count": 46, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "# write a summary of Tsinghua High School.  You can use any tool ont the built-in tool page or found on the Internet.\n", "# see what could be wrong with the answer?\n"]}], "metadata": {"kernelspec": {"display_name": ".venv", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.13"}}, "nbformat": 4, "nbformat_minor": 2}