{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Lab 8 Supervised Fine Tuning\n", "\n", "In this lab, we will perform\u00a0parameter efficient finetuning (PEFT) to finetune\u00a0a llama-2\u00a0model, using the HuggingFace SFTTrainer tool from its trl library.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 0. Dependencies and compatibility"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["!pip install -r requirements.txt"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["# Test whether your GPU supports bfloat16\n", "import torch\n", "major, _ = torch.cuda.get_device_capability()\n", "if major >= 8:\n", "    print(\"\"\"Your GPU supports bfloat16: you can accelerate training by setting \n", "          bnb_4bit_compute_dtype to torch.bfloat16 and bf16 in the trainer to True\"\"\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Prepare the dataset\n", "\n", "See the notebook 01_prepare_data.ipynb\n", "\n", "Make sure that you generate the correct dataset using that notebook before continuing. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Get the original inference results (baseline)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here we perform an inference on the original llama-2 model."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.1 Setting up the inference (same as previous labs)"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["################################################################################\n", "# Shared parameters between inference and SFT training\n", "################################################################################\n", "\n", "# The base model\n", "model_name = \"/share/model/llama-2-7b-chat-hf\"\n", "# Use a single GPU\n", "# device_map = {'':0}\n", "# Use all GPUs\n", "device_map = \"auto\"\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["################################################################################\n", "# bitsandbytes parameters\n", "################################################################################\n", "from transformers import BitsAndBytesConfig\n", "\n", "bnb_config = BitsAndBytesConfig(\n", "    load_in_4bit= True,    # use 4-bit precision for base model loading\n", "    bnb_4bit_quant_type= \"nf4\",  # Quantization type (fp4 or nf4)\n", "    bnb_4bit_compute_dtype= torch.bfloat16,   # Compute dtype for 4-bit base models  \"float16\" or torch.bfloat16\n", "    bnb_4bit_use_double_quant= False,  # Activate nested quantization for 4-bit base models (double quantization)\n", ")"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["from transformers import (\n", "    AutoModelForCausalLM,\n", "    AutoTokenizer,\n", "    TrainingArguments,\n", "    pipeline,\n", ")\n", "\n", "# Load base model with bnb config\n", "model = AutoModelForCausalLM.from_pretrained(\n", "    model_name,\n", "    quantization_config=bnb_config,\n", "    device_map=device_map\n", ")\n", "model.config.use_cache = False\n", "model.config.pretraining_tp = 1\n", "\n", "# Load LLaMA tokenizer\n", "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n", "tokenizer.pad_token = tokenizer.eos_token\n", "tokenizer.padding_side = \"left\"\n"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["# Run text generation pipeline with our next model\n", "prompt = \"Write a Python function to return the mode (the value or values that appear most frequently within that list) in a given list. If there are multiple modes, return -1. You should generate a function:\\n def solve(list):\"\n", "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n", "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n", "print(result[0]['generated_text'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.2 Evaluating the code generated"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["# Using regrex to capture the generated Python to a string\n", "import re\n", "\n", "def extract_first_code_snippet(text):\n", "    # Use a regular expression to find the first code snippet enclosed in triple backticks\n", "    match = re.search(r\"```(.*?)```\", text, re.S)\n", "    if match:\n", "        # Return the first matched group, which is the content within the backticks\n", "        return match.group(1)\n", "    else:\n", "        # Return None if no match is found\n", "        return None\n", "    \n", "code = extract_first_code_snippet(result[0]['generated_text'])\n", "print(code)"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["# Define a testcase using the standard python unittest library\n", "\n", "import unittest\n", "\n", "# place holder for the AI generated code\n", "def solve(list):\n", "    return 0  \n", "\n", "class TestGeneratedCode(unittest.TestCase):\n", "\n", "    def test_no_single_mode(self):\n", "        self.assertEqual(solve([3, 2, 1]), -1)\n", "\n", "    def test_single_mode(self):\n", "        self.assertEqual(solve([4, 9, 2, 33, 2]), 2)\n", "\n", "    def test_no_single_mode_3(self):\n", "        self.assertEqual(solve([7, 9, 11, 323, 996]), -1)\n", "\n", "def run_all_tests():\n", "    unittest.main(argv=[''], verbosity=2, exit=False) "]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["exec(code)  # run the generated code to redefine solve() function\n", "run_all_tests() # Expect to fail"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Finetune the Llama2 model\n", "In this section, we will finetune a chat model. We choose Llama-chat-7B as our base model and our dataset is code instrcutions, which guides LLM to write programming code from given natural language. Don't panic since this is not a difficult task and we will separate this task into several procedures."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.1  Set training arguments\n", "In this subsection, you need to read the given code snippets below. If you have some questions, you can either refer to the official documents or discuss with TAs or you classmates."]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["################################################################################\n", "# Model name and directories\n", "################################################################################\n", "\n", "# The base model\n", "model_name = \"/share/model/llama-2-7b-chat-hf\"\n", "# The instruction dataset to use\n", "dataset_name = \"/scratch2/py18k\"\n", "# Fine-tuned model name\n", "new_model = \"/scratch2/llama-2-7b-py18k\"\n", "# Output directory where the model predictions and checkpoints will be stored\n", "output_dir = \"/scratch2/results\"\n", "\n", "################################################################################\n", "# QLoRA parameters\n", "################################################################################\n", "\n", "# LoRA attention dimension\n", "lora_r = 64\n", "# Alpha parameter for LoRA scaling\n", "lora_alpha = 16\n", "# Dropout probability for LoRA layers\n", "lora_dropout = 0.1\n", "bias=\"none\"\n", "task_type=\"CAUSAL_LM\"\n", "\n", "################################################################################\n", "# Training parameters (passed to TrainingArguments)\n", "################################################################################\n", "\n", "# Number of training epochs\n", "num_train_epochs = 1\n", "# Number of training steps (overrides num_train_epochs)\n", "# max_steps = 100\n", "# Enable fp16/bf16 training (set bf16 to True if supported by your GPU)\n", "fp16 = False\n", "bf16 = True\n", "# Batch size per GPU for training\n", "per_device_train_batch_size = 4\n", "# Batch size per GPU for evaluation\n", "per_device_eval_batch_size = 4\n", "# Number of update steps to accumulate the gradients for\n", "gradient_accumulation_steps = 1\n", "# Enable gradient checkpointing\n", "gradient_checkpointing = True\n", "# Maximum gradient normal (gradient clipping)\n", "max_grad_norm = 0.3\n", "# Initial learning rate (AdamW optimizer)\n", "learning_rate = 2e-4\n", "# Weight decay to apply to all layers except bias/LayerNorm weights\n", "weight_decay = 0.001\n", "# Optimizer to use\n", "optim = \"paged_adamw_32bit\"\n", "# Learning rate schedule\n", "lr_scheduler_type = \"cosine\"\n", "# Ratio of steps for a linear warmup (from 0 to learning rate)\n", "warmup_ratio = 0.03\n", "# Group sequences into batches with same length\n", "# Saves memory and speeds up training considerably\n", "group_by_length = True\n", "# Save checkpoint every X updates steps\n", "save_steps = 0\n", "\n", "################################################################################\n", "# Monitoring parameters\n", "################################################################################\n", "\n", "# Logging dir (for tensorboard)\n", "logging_dir = f\"{output_dir}/logs\"\n", "# Log every X updates steps\n", "logging_steps = 25\n", "# Monitoring and Visualizing tools\n", "report_to = \"tensorboard\"\n", "\n", "################################################################################\n", "# SFTTrainer parameters\n", "################################################################################\n", "\n", "# Maximum sequence length to use\n", "max_seq_length = 512\n", "# Pack multiple short examples in the same input sequence to increase efficiency\n", "packing = False\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.2 Load Training Dataset (prepared in section 1)"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["# Load dataset\n", "from datasets import load_from_disk\n", "dataset = load_from_disk(dataset_name)\n", "print(dataset[0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.3 Finetuning the model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.3.1 Construct the configuration objects"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["from peft import PeftModel\n", "from trl import SFTTrainer\n", "\n", "# Set training parameters\n", "training_arguments = TrainingArguments(\n", "    output_dir=output_dir,\n", "    num_train_epochs=num_train_epochs,\n", "    #max_steps=max_steps,\n", "    per_device_train_batch_size=per_device_train_batch_size,\n", "    gradient_accumulation_steps=gradient_accumulation_steps,\n", "    optim=optim,\n", "    save_steps=save_steps,\n", "    learning_rate=learning_rate,\n", "    weight_decay=weight_decay,\n", "    fp16=fp16,\n", "    bf16=bf16,\n", "    max_grad_norm=max_grad_norm,\n", "    warmup_ratio=warmup_ratio,\n", "    group_by_length=group_by_length,\n", "    lr_scheduler_type=lr_scheduler_type,\n", "    logging_steps=logging_steps,\n", "    logging_dir=logging_dir,\n", "    report_to=report_to, \n", ")"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["from peft import LoraConfig\n", "peft_config = LoraConfig(\n", "    lora_alpha=lora_alpha,\n", "    lora_dropout=lora_dropout,\n", "    r=lora_r,\n", "    bias=bias,\n", "    task_type=task_type,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.3.2 Initialize SFTTrainer"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["# Set supervised fine-tuning parameters\n", "trainer = SFTTrainer(\n", "    model=model,\n", "    train_dataset=dataset,\n", "    peft_config=peft_config,\n", "    dataset_text_field=\"text\",\n", "    max_seq_length=max_seq_length,\n", "    tokenizer=tokenizer,\n", "    args=training_arguments,\n", "    packing=packing,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.3.3 start the tensorboard for monitoring"]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": ["import tensorboard\n", "\n", "# Visualize the finetuning process.\n", "#%load_ext tensorboard\n", "%reload_ext tensorboard\n", "%tensorboard --logdir /scratch2/results/logs"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.3.4 Train the model (takes about half an hour on 4090)"]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": ["# # Train model\n", "trainer.train()\n", "\n", "# Save trained model\n", "trainer.model.save_pretrained(new_model)"]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": ["# release GPU memory here\n", "\n", "torch.cuda.reset_peak_memory_stats()\n", "torch.cuda.empty_cache()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.3.5 Merge Lora_model with Base model and save the merged model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You should first export Lora model with base model and convert them into hf checkpoint. \n", "This makes up the final trained merged model."]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["# Reload model in FP16 and merge it with LoRA weights\n", "base_model = AutoModelForCausalLM.from_pretrained(\n", "    model_name,\n", "    low_cpu_mem_usage=True,\n", "    return_dict=True,\n", "    torch_dtype=torch.float16,\n", "    device_map=device_map,\n", ")\n", "model = PeftModel.from_pretrained(base_model, new_model)\n", "merged_model = model.merge_and_unload()\n", "\n", "# Reload tokenizer to save it\n", "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n", "tokenizer.pad_token = tokenizer.eos_token\n", "tokenizer.padding_side = \"left\"\n", "\n", "# Save merged model to disk (optional)\n", "merged_model.save_pretrained(f'{new_model}_merged')\n"]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["# (Optional) Push Lora model to Huggingface hub (optional)\n", "# !huggingface-cli login\n", "\n", "# merged_model.push_to_hub(new_model, use_temp_dir=False)\n", "# tokenizer.push_to_hub(new_model, use_temp_dir=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.6 examine the results in tensorboard\n"]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": ["# set up port forwarding in vscode\n", "# open the tensorboard page at http://localhost:6006"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.7 Test the SFTed model"]}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [], "source": ["# Run text generation pipeline with our next model\n", "prompt = \"Write a Python function to return the mode (the value or values that appear most frequently within that list) in a given list. If there are multiple modes, return -1. You should generate a function:\\n def solve(list):\"\n", "pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer, max_length=512)\n", "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n", "print(result[0]['generated_text'])"]}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [], "source": ["code = extract_first_code_snippet(result[0]['generated_text'])\n", "print(code)\n"]}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [], "source": ["exec(code)\n", "run_all_tests() # Expect to pass"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.13"}, "vscode": {"interpreter": {"hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}}, "nbformat": 4, "nbformat_minor": 4}